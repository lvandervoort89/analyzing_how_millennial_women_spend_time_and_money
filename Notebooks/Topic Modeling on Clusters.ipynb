{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing how millennial women spend money via Refinery29\n",
    "**Objective**   \n",
    "Analyze how millennial women spend their time and money using NLP. Build a recommender that takes in user input and selects 3 Refinery29 Money Diaries that are similar to the user.  \n",
    "\n",
    "**Data**   \n",
    "This data was scraped from the [Refinery29 Money Diaries](https://refinery29.com/en-us/money-diary) from January 18, 2019-June 3, 2020.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:54:58.048103Z",
     "start_time": "2020-08-25T02:54:58.040233Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim import matutils, models\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load in data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:08.004433Z",
     "start_time": "2020-08-25T02:55:07.960324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load in pickled dataframes\n",
    "clustered_diarist_df = pickle.load(open(\"clustered_data_scaled.pkl\",\"rb\"))\n",
    "text_df = pickle.load(open(\"text_df.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:08.523935Z",
     "start_time": "2020-08-25T02:55:08.515421Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge dataframes to append clusters to text\n",
    "updated_text_df = pd.merge(clustered_diarist_df, text_df, left_on='story_title', right_on='story_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:08.842892Z",
     "start_time": "2020-08-25T02:55:08.838280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns from new dataframe\n",
    "updated_text_df.drop(columns=['occupation', 'age', 'salary', 'nomad', 'international', 'high_cost_of_living_area'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:11.812882Z",
     "start_time": "2020-08-25T02:55:11.787184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the diary text column from a list to a string\n",
    "updated_text_df['diary_text_string'] = [', '.join(map(str, l)) for l in updated_text_df['diary_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:12.074564Z",
     "start_time": "2020-08-25T02:55:12.071893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the diary_text as list column\n",
    "del updated_text_df['diary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:12.368697Z",
     "start_time": "2020-08-25T02:55:12.363112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation, remove words \n",
    "    containing numbers, remove additional punctuation and other non-sensical text.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(' — ',' ', text)      #attempts to remove hyphen\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)   #removes numbers\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('^', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:14.054990Z",
     "start_time": "2020-08-25T02:55:12.652764Z"
    }
   },
   "outputs": [],
   "source": [
    "updated_text_df.diary_text_string = updated_text_df.diary_text_string.apply(round1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create separate dataframes for each cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:14.880291Z",
     "start_time": "2020-08-25T02:55:14.875598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster 0: Late 20s, average earners\n",
    "cluster_0 = updated_text_df[updated_text_df['Cluster'] == 0]\n",
    "\n",
    "# Drop the cluster column\n",
    "del cluster_0['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:15.170651Z",
     "start_time": "2020-08-25T02:55:15.167530Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster 1: Late 30's, average earners\n",
    "cluster_1 = updated_text_df[updated_text_df['Cluster'] == 1]\n",
    "\n",
    "# Drop the cluster column\n",
    "del cluster_1['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:15.470585Z",
     "start_time": "2020-08-25T02:55:15.467418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster 2: early 20s, average starting salary \n",
    "cluster_2 = updated_text_df[updated_text_df['Cluster'] == 2]\n",
    "\n",
    "# Drop the cluster column\n",
    "del cluster_2['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:15.785806Z",
     "start_time": "2020-08-25T02:55:15.781519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster 3: Mid-30s high earners \n",
    "cluster_3 = updated_text_df[updated_text_df['Cluster'] == 3]\n",
    "\n",
    "# Drop the cluster column\n",
    "del cluster_3['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:16.728124Z",
     "start_time": "2020-08-25T02:55:16.724573Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster 4: Late 20s, high earners\n",
    "cluster_4 = updated_text_df[updated_text_df['Cluster'] == 4]\n",
    "\n",
    "# Drop the cluster column\n",
    "del cluster_4['Cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create helper functions to evaluate models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:32.243986Z",
     "start_time": "2020-08-25T02:55:32.240237Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    \"Given a model, return the top words for each topic\"\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:32.528091Z",
     "start_time": "2020-08-25T02:55:32.524849Z"
    }
   },
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:55:32.828915Z",
     "start_time": "2020-08-25T02:55:32.823911Z"
    }
   },
   "outputs": [],
   "source": [
    "# Updated list of stop words\n",
    "additional_stop_words = ['pm', 'im', 'day', 'week', 'total', 'daily', 'like', 'today', 'dont', 'just', 'women',\n",
    "                        'way', 'really', 'ive', 'diaries', 'got', 'gets', 'ill', 'things', 'bit', 'sevenday',\n",
    "                        'hes', 'let', 'shes', 'lot', 'little', 'decide', 'ready', 'feel', 'goes', 'stop',\n",
    "                        'finally', 'welcome', 'money', 'diaries', 'period', 'job', 'share', 'today', 'millennials',\n",
    "                        'occupation', 'paycheck', 'diaries', 'taboo', 'dollar','period', 'year', 'month', 'worth',\n",
    "                         'rent', 'bonus', 'expenses', 'today', 'mortgage', 'debt', 'expensesmortgage', 'industry',\n",
    "                        'expensesrent', 'salary', 'savings', 'insurance', 'loan', 'loans', 'income', 'student',\n",
    "                        'dollartoday', 'cash', 'room', 'account', 'woman', 'living','house', 'housing','mortgagepaycheck',\n",
    "                        'half','split', 'apartment', 'totaldebt', 'checksavingscd', 'stuff', 'hour', 'hours',\n",
    "                        'people', 'place', 'years', 'minutes', 'tomorrow', 'couple', 'head', 'meeting']\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(additional_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic modeling cluster 0 (late 20s, average earners)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling exploration included all variations of the following.\n",
    "- LDA, LSA, NMF\n",
    "- CountVectorizer and TfidfVectorizer\n",
    "- Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "- Lemmatization and stemming\n",
    "- Parts of speech: nouns only, verbs only, and nouns + verbs  \n",
    "\n",
    "Optimal condition for Topic 0:\n",
    "- NMF with CountVectorizer\n",
    "- Parts of speech: nouns only\n",
    "- Additional stop words\n",
    "- Max_df = 88, min_df = .15\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:28:00.092841Z",
     "start_time": "2020-08-20T23:27:38.414709Z"
    }
   },
   "outputs": [],
   "source": [
    "data_nouns_cluster_0 = pd.DataFrame(cluster_0.diary_text_string.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:28:00.427790Z",
     "start_time": "2020-08-20T23:28:00.173796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 651)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvn0 = CountVectorizer(stop_words=stop_words, min_df = 0.15, max_df=.88, ngram_range=(1, 2))\n",
    "data_cvn0 = cvn0.fit_transform(data_nouns_cluster_0.diary_text_string)\n",
    "data_dtmn0 = pd.DataFrame(data_cvn0.toarray(), columns=cvn0.get_feature_names())\n",
    "data_dtmn0.index = data_nouns_cluster_0.index\n",
    "data_dtmn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:28:00.605808Z",
     "start_time": "2020-08-20T23:28:00.539853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "food, breakfast, walk, days, school, routine, break, phone, kids, hair\n",
      "\n",
      "Topic  1\n",
      "dog, walk, dogs, breakfast, office, home dog, coffee, dog walk, friends, bar\n",
      "\n",
      "Topic  2\n",
      "husband, chicken, breakfast, office, food, husbands, salad, shower, pizza, car\n",
      "\n",
      "Topic  3\n",
      "baby, milk, coffee, downstairs, granola, bowl, mom, yogurt, cup, shower\n",
      "\n",
      "Topic  4\n",
      "coffee, office, cup, days, class, cat, breakfast, store, butter, episode\n",
      "\n",
      "Topic  5\n",
      "friend, friends, office, bar, food, alarm, class, card, water, party\n"
     ]
    }
   ],
   "source": [
    "nmf_model_cluster_0 = NMF(6)\n",
    "doc_topic0 = nmf_model_cluster_0.fit_transform(data_dtmn0)\n",
    "display_topics(nmf_model_cluster_0, cvn0.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:28:00.754880Z",
     "start_time": "2020-08-20T23:28:00.750972Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_0_component_matrix = pd.DataFrame(doc_topic0.round(5),\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', \"component_4\", 'compnent_5',\n",
    "                     'component_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T20:13:47.409320Z",
     "start_time": "2020-08-23T20:13:47.396440Z"
    }
   },
   "source": [
    "Based on these results, the topics were determined to be:\n",
    "- Home routines\n",
    "- Dogs\n",
    "- Husband\n",
    "- Baby/mom\n",
    "- Work\n",
    "- Friends/socializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:12:21.970910Z",
     "start_time": "2020-08-21T00:12:21.862880Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle important things for recommender\n",
    "# with open('cvn0.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvn0, f)\n",
    "\n",
    "# with open('doc_topic0.pkl', 'wb') as f:\n",
    "#     pickle.dump(doc_topic0, f)\n",
    "    \n",
    "# with open('nmf_model_cluster_0.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmf_model_cluster_0, f)\n",
    "    \n",
    "# with open('cluster_0.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_0, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic modeling cluster 1 (late 30's, average earners)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling exploration included all variations of the following.\n",
    "- LDA, LSA, NMF\n",
    "- CountVectorizer and TfidfVectorizer\n",
    "- Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "- Lemmatization and stemming\n",
    "- Parts of speech: nouns only, verbs only, and nouns + verbs  \n",
    "\n",
    "Optimal condition for Topic 1:\n",
    "- NMF with CountVectorizer\n",
    "- Parts of speech: nouns only\n",
    "- Additional stop words cluster 1 specific\n",
    "- Max_df = .7, min_df = .12\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:22.200488Z",
     "start_time": "2020-08-21T00:08:15.981556Z"
    }
   },
   "outputs": [],
   "source": [
    "data_nouns_cluster_1 = pd.DataFrame(cluster_1.diary_text_string.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:24.393809Z",
     "start_time": "2020-08-21T00:08:24.386239Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_stop_words_cluster_1 = ['pm', 'im', 'day', 'week', 'total', 'daily', 'like', 'today', 'dont', 'just', 'women',\n",
    "                        'way', 'really', 'ive', 'diaries', 'got', 'gets', 'ill', 'things', 'bit', 'sevenday',\n",
    "                        'hes', 'let', 'shes', 'lot', 'little', 'decide', 'ready', 'feel', 'goes', 'stop',\n",
    "                        'finally', 'welcome', 'money', 'diaries', 'period', 'job', 'share', 'today', 'millennials',\n",
    "                        'occupation', 'paycheck', 'diaries', 'taboo', 'dollar','period', 'year', 'month', 'worth',\n",
    "                         'rent', 'bonus', 'expenses', 'today', 'mortgage', 'debt', 'expensesmortgage', 'industry',\n",
    "                        'expensesrent', 'salary', 'savings', 'insurance', 'loan', 'loans', 'income', 'student',\n",
    "                        'dollartoday', 'cash', 'room', 'account', 'woman', 'living','house', 'housing','mortgagepaycheck',\n",
    "                        'half','split', 'apartment', 'totaldebt', 'checksavingscd', 'stuff', 'hour', 'hours',\n",
    "                        'people', 'place', 'years', 'minutes', 'tomorrow', 'couple', 'head', 'morning',\n",
    "                        'night', 'guy']\n",
    "\n",
    "stop_words_cluster_1 = text.ENGLISH_STOP_WORDS.union(additional_stop_words_cluster_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:25.544575Z",
     "start_time": "2020-08-21T00:08:25.449543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 904)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvn1 = CountVectorizer(stop_words=stop_words_cluster_1, min_df = 0.12, max_df=.7, ngram_range=(1, 2))\n",
    "data_cvn1 = cvn1.fit_transform(data_nouns_cluster_1.diary_text_string)\n",
    "data_dtmn1 = pd.DataFrame(data_cvn1.toarray(), columns=cvn1.get_feature_names())\n",
    "data_dtmn1.index = data_nouns_cluster_1.index\n",
    "data_dtmn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:26.388530Z",
     "start_time": "2020-08-21T00:08:26.327361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "wine, bar, desk, alarm, train, class, chicken, yogurt, date, kitchen\n",
      "\n",
      "Topic  1\n",
      "kids, partner, school, boys, daycare, commute, paper, daughter, game, lunches\n",
      "\n",
      "Topic  2\n",
      "husband, boys, son, husbands, rice, desk, email, yogurt, meeting, bar\n",
      "\n",
      "Topic  3\n",
      "girls, school, hair, gym, chicken, husband, meeting, parents, project, guest\n",
      "\n",
      "Topic  4\n",
      "dog, business, cats, boyfriend, airport, town, dogs, tea, lots, tacos\n",
      "\n",
      "Topic  5\n",
      "baby, banana, pizza, chicken, milk, sister, strawberries, cheese, daycare, butter\n"
     ]
    }
   ],
   "source": [
    "nmf_model_cluster_1 = NMF(6)\n",
    "doc_topic1 = nmf_model_cluster_1.fit_transform(data_dtmn1)\n",
    "display_topics(nmf_model_cluster_1, cvn1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:30.490915Z",
     "start_time": "2020-08-21T00:08:30.487977Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_1_component_matrix = pd.DataFrame(doc_topic1.round(5),\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', \"component_4\", 'compnent_5',\n",
    "                     'component_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the topics were determined to be:\n",
    "- Work\n",
    "- Kid routines\n",
    "- Family\n",
    "- Adult routines\n",
    "- Pets\n",
    "- Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:08:30.940184Z",
     "start_time": "2020-08-21T00:08:30.923388Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle important things for recommender\n",
    "# with open('cvn1.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvn1, f)\n",
    "\n",
    "# with open('doc_topic1.pkl', 'wb') as f:\n",
    "#     pickle.dump(doc_topic1, f)\n",
    "    \n",
    "# with open('nmf_model_cluster_1.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmf_model_cluster_1, f)\n",
    "    \n",
    "# with open('cluster_1.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic modeling for cluster 2 (early 20s, entry level salary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling exploration included all variations of the following.\n",
    "- LDA, LSA, NMF\n",
    "- CountVectorizer and TfidfVectorizer\n",
    "- Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "- Lemmatization and stemming\n",
    "- Parts of speech: nouns only, verbs only, and nouns + verbs  \n",
    "\n",
    "Optimal condition for Topic 2:\n",
    "- NMF with CountVectorizer\n",
    "- Parts of speech: nouns only\n",
    "- Additional stop words \n",
    "- Max_df = .8, min_df = .1\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:56:11.149188Z",
     "start_time": "2020-08-25T02:55:43.393454Z"
    }
   },
   "outputs": [],
   "source": [
    "data_nouns_cluster_2 = pd.DataFrame(cluster_2.diary_text_string.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:56:11.564494Z",
     "start_time": "2020-08-25T02:56:11.230919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 960)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate a document-term matrix with only nouns\n",
    "cvn2 = CountVectorizer(stop_words=stop_words, min_df = 0.1, max_df=.8, ngram_range=(1, 2))\n",
    "data_cvn2 = cvn2.fit_transform(data_nouns_cluster_2.diary_text_string)\n",
    "data_dtmn2 = pd.DataFrame(data_cvn2.toarray(), columns=cvn2.get_feature_names())\n",
    "data_dtmn2.index = data_nouns_cluster_2.index\n",
    "data_dtmn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T02:56:11.936354Z",
     "start_time": "2020-08-25T02:56:11.700695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "mom, parents, dad, car, family, weekend, brother, water, pay, door\n",
      "\n",
      "Topic  1\n",
      "sister, school, card, routine, book, credit, kids, credit card, kitchen, bacon\n",
      "\n",
      "Topic  2\n",
      "dog, walk, dogs, car, cream, business, school, dog walk, office, ice cream\n",
      "\n",
      "Topic  3\n",
      "husband, kids, water, tea, chicken, table, media, café, cream, pajamas\n",
      "\n",
      "Topic  4\n",
      "class, school, bus, studio, classes, yoga, gym, alarm, workout, rest\n",
      "\n",
      "Topic  5\n",
      "partner, airport, hotel, restaurant, flight, shop, water, card, pay, flights\n",
      "\n",
      "Topic  6\n",
      "office, bar, chicken, salad, train, boss, desk, door, tonight, alarm\n",
      "\n",
      "Topic  7\n",
      "hair, walk, episode, gym, water, face, teeth, routine, butter, makeup\n"
     ]
    }
   ],
   "source": [
    "nmf_model_cluster_2 = NMF(8)\n",
    "doc_topic2 = nmf_model_cluster_2.fit_transform(data_dtmn2)\n",
    "display_topics(nmf_model_cluster_2, cvn2.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the topics were determined to be:\n",
    "- Self-care\n",
    "- Purcahses\n",
    "- Dogs/pets\n",
    "- Food\n",
    "- Fitness\n",
    "- Travel\n",
    "- Work\n",
    "- Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-20T23:42:29.976581Z",
     "start_time": "2020-08-20T23:42:29.903942Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle important things for recommender\n",
    "# with open('cvn2.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvn2, f)\n",
    "\n",
    "# with open('doc_topic2.pkl', 'wb') as f:\n",
    "#     pickle.dump(doc_topic2, f)\n",
    "    \n",
    "# with open('nmf_model_cluster_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmf_model_cluster_2, f)\n",
    "    \n",
    "# with open('cluster_2.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic modeling for cluster 3 (mid 30s, high earners)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling exploration included all variations of the following.\n",
    "- LDA, LSA, NMF\n",
    "- CountVectorizer and TfidfVectorizer\n",
    "- Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "- Lemmatization and stemming\n",
    "- Parts of speech: nouns only, verbs only, and nouns + verbs  \n",
    "\n",
    "Optimal condition for Topic 3:\n",
    "- NMF with CountVectorizer\n",
    "- Parts of speech: nouns only\n",
    "- Additional stop words cluster 3 specific \n",
    "- Min_df = .26\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:07.502339Z",
     "start_time": "2020-08-21T00:13:07.003389Z"
    }
   },
   "outputs": [],
   "source": [
    "data_nouns_cluster_3 = pd.DataFrame(cluster_3.diary_text_string.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:08.283681Z",
     "start_time": "2020-08-21T00:13:08.276289Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_stop_words_cluster3 = ['pm', 'im', 'day', 'week', 'total', 'daily', 'like', 'today', 'dont', 'just', 'women',\n",
    "                        'way', 'really', 'ive', 'diaries', 'got', 'gets', 'ill', 'things', 'bit', 'sevenday',\n",
    "                        'hes', 'let', 'shes', 'lot', 'little', 'decide', 'ready', 'feel', 'goes', 'stop',\n",
    "                        'finally', 'welcome', 'money', 'diaries', 'period', 'job', 'share', 'today', 'millennials',\n",
    "                        'occupation', 'paycheck', 'diaries', 'taboo', 'dollar','period', 'year', 'month', 'worth',\n",
    "                         'rent', 'bonus', 'expenses', 'today', 'mortgage', 'debt', 'expensesmortgage', 'industry',\n",
    "                        'expensesrent', 'salary', 'savings', 'insurance', 'loan', 'loans', 'income', 'student',\n",
    "                        'dollartoday', 'cash', 'room', 'account', 'woman', 'living','house', 'housing','mortgagepaycheck',\n",
    "                        'half','split', 'apartment', 'totaldebt', 'checksavingscd', 'stuff', 'hour', 'hours',\n",
    "                        'people', 'place', 'years', 'minutes', 'tomorrow', 'couple', 'head', 'morning',\n",
    "                        'night', 'guy', 'lunch', 'dinner', 'breakfast', 'time']\n",
    "\n",
    "stop_words_cluster3 = text.ENGLISH_STOP_WORDS.union(additional_stop_words_cluster3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:08.749897Z",
     "start_time": "2020-08-21T00:13:08.736464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 327)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvn3 = CountVectorizer(stop_words=stop_words_cluster3,  min_df = .26, ngram_range=(1, 2))\n",
    "data_cvn3 = cvn3.fit_transform(data_nouns_cluster_3.diary_text_string)\n",
    "data_dtmn3 = pd.DataFrame(data_cvn3.toarray(), columns=cvn3.get_feature_names())\n",
    "data_dtmn3.index = data_nouns_cluster_3.index\n",
    "data_dtmn3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:09.746936Z",
     "start_time": "2020-08-21T00:13:09.735460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "home, school, husband, work, coffee, order, door, dog, shopping, bed\n",
      "\n",
      "Topic  1\n",
      "kids, business, order, coffee, estate, meeting, phone, emails, amazon, city\n",
      "\n",
      "Topic  2\n",
      "work, home, coffee, wedding, family, dress, bed, mom, kitchen, service\n",
      "\n",
      "Topic  3\n",
      "work, dogs, walk, wine, order, break, list, delivery, city, online\n",
      "\n",
      "Topic  4\n",
      "home, dogs, school, work, bed, wine, life, sister, gym, mom\n"
     ]
    }
   ],
   "source": [
    "nmf_model_cluster_3 = NMF(5)\n",
    "doc_topic3 = nmf_model_cluster_3.fit_transform(data_dtmn3)\n",
    "display_topics(nmf_model_cluster_3, cvn3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:12.880902Z",
     "start_time": "2020-08-21T00:13:12.877102Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_3_component_matrix = pd.DataFrame(doc_topic3.round(5),\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', \"component_4\", 'compnent_5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the topics were determined to be:\n",
    "- Family routines\n",
    "- Work\n",
    "- Parenting\n",
    "- Self-care\n",
    "- Family "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:13.247047Z",
     "start_time": "2020-08-21T00:13:13.239919Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle important things for recommender\n",
    "# with open('cvn3.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvn3, f)\n",
    "\n",
    "# with open('doc_topic3.pkl', 'wb') as f:\n",
    "#     pickle.dump(doc_topic3, f)\n",
    "    \n",
    "# with open('nmf_model_cluster_3.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmf_model_cluster_3, f)\n",
    "    \n",
    "# with open('cluster_3.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Topic modeling for cluster 4 (late 20s, high earners)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling exploration included all variations of the following.\n",
    "- LDA, LSA, NMF\n",
    "- CountVectorizer and TfidfVectorizer\n",
    "- Hyperparameters including stop words, max_df, min_df, and ngram_range\n",
    "- Lemmatization and stemming\n",
    "- Parts of speech: nouns only, verbs only, and nouns + verbs  \n",
    "\n",
    "Optimal condition for Topic 4:\n",
    "- NMF with CountVectorizer\n",
    "- Parts of speech: nouns only\n",
    "- Additional stop words \n",
    "- Max_df = .92, min_df = .19\n",
    "- Unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:49.930799Z",
     "start_time": "2020-08-21T00:13:43.368201Z"
    }
   },
   "outputs": [],
   "source": [
    "data_nouns_cluster_4 = pd.DataFrame(cluster_4.diary_text_string.apply(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:50.238870Z",
     "start_time": "2020-08-21T00:13:50.157463Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 556)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvn4 = CountVectorizer(stop_words=stop_words, min_df = 0.19,max_df=.92, ngram_range=(1, 2))\n",
    "data_cvn4 = cvn4.fit_transform(data_nouns_cluster_4.diary_text_string)\n",
    "data_dtmn4 = pd.DataFrame(data_cvn4.toarray(), columns=cvn4.get_feature_names())\n",
    "data_dtmn4.index = data_nouns_cluster_4.index\n",
    "data_dtmn4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:50.559827Z",
     "start_time": "2020-08-21T00:13:50.503108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "coffee, friends, friend, bar, card, salad, book, date, store, drinks\n",
      "\n",
      "Topic  1\n",
      "office, baby, shower, husband, company, emails, party, food, mom, car\n",
      "\n",
      "Topic  2\n",
      "butter, parents, cream, meetings, walk, chocolate, ice, break, weekend, ice cream\n",
      "\n",
      "Topic  3\n",
      "husband, dog, food, office, wine, store, coffee, family, car, husbands\n",
      "\n",
      "Topic  4\n",
      "gym, office, friend, water, breakfast, desk, food, workout, days, card\n",
      "\n",
      "Topic  5\n",
      "kids, clothes, door, coffee, team, friends, breakfast, chicken, office, family\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisavandervoort/opt/anaconda3/envs/metis/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "nmf_model_cluster_4 = NMF(6)\n",
    "doc_topic4 = nmf_model_cluster_4.fit_transform(data_dtmn4)\n",
    "display_topics(nmf_model_cluster_4, cvn4.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:50.879996Z",
     "start_time": "2020-08-21T00:13:50.875892Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster_4_component_matrix = pd.DataFrame(doc_topic4.round(5),\n",
    "             columns = [\"component_1\",\"component_2\", 'component_3', \"component_4\", 'compnent_5', 'compnent_6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these results, the topics were determined to be:\n",
    "- Dating\n",
    "- Parties\n",
    "- Food\n",
    "- Home\n",
    "- Fitness\n",
    "- Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-21T00:13:51.156099Z",
     "start_time": "2020-08-21T00:13:51.142168Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Pickle important things for recommender\n",
    "# with open('cvn4.pkl', 'wb') as f:\n",
    "#     pickle.dump(cvn4, f)\n",
    "\n",
    "# with open('doc_topic4.pkl', 'wb') as f:\n",
    "#     pickle.dump(doc_topic4, f)\n",
    "    \n",
    "# with open('nmf_model_cluster_4.pkl', 'wb') as f:\n",
    "#     pickle.dump(nmf_model_cluster_4, f)\n",
    "    \n",
    "# with open('cluster_4.pkl', 'wb') as f:\n",
    "#     pickle.dump(cluster_4, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
