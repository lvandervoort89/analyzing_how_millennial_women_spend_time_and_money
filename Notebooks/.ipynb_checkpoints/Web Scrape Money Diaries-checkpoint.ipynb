{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing how millennial women spend money via Refinery29\n",
    "**Objective**   \n",
    "Analyze how millennial women spend their time and money using NLP. Build a recommender that takes in user input and selects 3 Refinery29 Money Diaries that are similar to the user.  \n",
    "\n",
    "**Data**   \n",
    "This data was scraped from the [Refinery29 Money Diaries](https://refinery29.com/en-us/money-diary) from January 18, 2019-June 3, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Selenium to get unique link identifiers for 500 money diaries. Pickle links and to put into Beautiful Soup.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\" \n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "website = 'https://www.refinery29.com/en-us/money-diary'\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_href_links = [i['href'] for i in soup.find_all('a', href=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_to_follow = find_href_links[22:-213]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle links to save for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('r29links.pkl', 'wb') as f:\n",
    "#     pickle.dump(links_to_follow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r29_links = pickle.load(open(\"r29links.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to get information about each diarist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diarist_value(soup, field_name):\n",
    "    \n",
    "    '''Grab a value from Money Diary. Takes a string attribute of a money diary and returns \n",
    "    the individual value'''\n",
    "    try:\n",
    "        obj = soup.find(text=re.compile(field_name))\n",
    "\n",
    "        if not obj: \n",
    "            return 'BLANK' \n",
    "        else:\n",
    "            individual_info = obj.next_element\n",
    "            if individual_info:\n",
    "                return individual_info.strip()\n",
    "            else:\n",
    "                return 'BLANK'\n",
    "    except TypeError:\n",
    "        return 'BLANK'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dictionary to hold the scraped data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_money_diaries_dict(id_link):\n",
    "    \n",
    "    '''Creates a dictionary of the categories of scraped data from each of the money diaries'''\n",
    "    \n",
    "    #Develop base URL\n",
    "    base_url = \"https://www.refinery29.com\"\n",
    "    \n",
    "    #Create full URL to scrape\n",
    "    url = base_url + id_link\n",
    "    \n",
    "    #Request HTML and parse\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "    headers = [\"story_title\", 'occupation', 'age', 'location', 'salary', 'net_worth', \n",
    "               'debt', 'rent', 'mortgage','loans', 'savings', 'diary_text']\n",
    "    \n",
    "    # Story title\n",
    "    story_title = id_link\n",
    "    \n",
    "    # Occupation\n",
    "    occupation = get_diarist_value(soup, 'Occupation:')\n",
    "    \n",
    "    # Age\n",
    "    age = get_diarist_value(soup, 'Age:')\n",
    "    \n",
    "    # Location\n",
    "    location = get_diarist_value(soup, 'Location:')\n",
    "    \n",
    "    # Salary\n",
    "    salary = get_diarist_value(soup, 'Salary:')\n",
    "    \n",
    "    # Net Worth\n",
    "    net_worth = get_diarist_value(soup, 'Net Worth:')\n",
    "    \n",
    "    # Debt\n",
    "    debt = get_diarist_value(soup, 'Debt:')\n",
    "    \n",
    "    # Rent\n",
    "    rent = get_diarist_value(soup, 'Rent:')\n",
    "    \n",
    "    # Mortgage\n",
    "    mortgage = get_diarist_value(soup, 'Mortgage:')\n",
    "    \n",
    "    # Loans\n",
    "    loans = get_diarist_value(soup, 'Loans:')\n",
    "    \n",
    "    # Savings\n",
    "    savings = get_diarist_value(soup, 'Savings:')\n",
    "    \n",
    "    # Diary text\n",
    "    diary_text = []\n",
    "    for div in soup.find_all('div', class_='section-text'):\n",
    "        diary_text.append(div.text)\n",
    "    \n",
    "    data_dict = dict(zip(headers, [story_title, occupation, age, location, \n",
    "                                   salary, net_worth, debt, rent, mortgage, \n",
    "                                   loans, savings, diary_text]))\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# money_diary_list = []\n",
    "\n",
    "# for link in r29_links:   \n",
    "#     money_diary_list.append(get_money_diaries_dict(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the list of scraped data to a dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_df = pd.DataFrame(money_diary_list)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create 2 dataframes- one with text daily diary and the other with diarist information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = money_df[['story_title','diary_text']]\n",
    "diarist_df = money_df.drop('diary_text',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pickle dataframes for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # money_df\n",
    "# with open('money_df.pkl', 'wb') as f:\n",
    "#     pickle.dump(money_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # text_df\n",
    "# with open('text_df.pkl', 'wb') as f:\n",
    "#     pickle.dump(text_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # diarist_df\n",
    "# with open('diarist_df.pkl', 'wb') as f:\n",
    "#     pickle.dump(diarist_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
